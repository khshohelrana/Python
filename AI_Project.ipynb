{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM2l8XzCbwXUBi2ct9PmQTB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khshohelrana/Python/blob/main/AI_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "import cv2\n",
        "import numpy as np\n",
        "import PIL\n",
        "from io import BytesIO\n",
        "from deepface import DeepFace\n",
        "\n",
        "def js_to_image(js_reply):\n",
        "    \"\"\"\n",
        "    Convert base64 image from javascript to an OpenCV image\n",
        "    \"\"\"\n",
        "    image_bytes = b64decode(js_reply.split(',')[1])\n",
        "    image = PIL.Image.open(BytesIO(image_bytes))\n",
        "    return cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)\n",
        "\n",
        "def take_photo(filename='photo.jpg', quality=0.8):\n",
        "    js = Javascript('''\n",
        "    async function takePhoto(quality) {\n",
        "      const div = document.createElement('div');\n",
        "      const capture = document.createElement('button');\n",
        "      capture.textContent = 'Capture';\n",
        "      document.body.appendChild(div);\n",
        "      div.appendChild(capture);\n",
        "\n",
        "      const video = document.createElement('video');\n",
        "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "      div.appendChild(video);\n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      // Resize the output to fit the video.\n",
        "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "\n",
        "      // Wait for Capture to be clicked.\n",
        "      await new Promise((resolve) => capture.onclick = resolve);\n",
        "\n",
        "      const canvas = document.createElement('canvas');\n",
        "      canvas.width = video.videoWidth;\n",
        "      canvas.height = video.videoHeight;\n",
        "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "      stream.getVideoTracks()[0].stop();\n",
        "      div.remove();\n",
        "      return canvas.toDataURL('image/jpeg', quality);\n",
        "    }\n",
        "    ''')\n",
        "    display(js)\n",
        "    data = eval_js('takePhoto({})'.format(quality))\n",
        "    img = js_to_image(data)\n",
        "    cv2.imwrite(filename, img)\n",
        "    return filename\n",
        "\n",
        "filename = take_photo()\n",
        "\n",
        "\n",
        "# Load face cascade classifier\n",
        "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "\n",
        "# Read the captured image\n",
        "frame = cv2.imread(filename)\n",
        "\n",
        "# Convert frame to grayscale\n",
        "gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "# Convert grayscale frame to RGB format\n",
        "rgb_frame = cv2.cvtColor(gray_frame, cv2.COLOR_GRAY2RGB)\n",
        "\n",
        "# Detect faces in the frame\n",
        "faces = face_cascade.detectMultiScale(gray_frame, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
        "\n",
        "for (x, y, w, h) in faces:\n",
        "    # Extract the face ROI (Region of Interest)\n",
        "    face_roi = rgb_frame[y:y + h, x:x + w]\n",
        "\n",
        "    # Perform emotion analysis on the face ROI\n",
        "    results = DeepFace.analyze(face_roi, actions=['emotion'], enforce_detection=False)\n",
        "\n",
        "    # Since DeepFace.analyze can return a list of results (for multiple faces),\n",
        "    # we'll assume you want the first face detected\n",
        "    if results:\n",
        "        result = results[0]\n",
        "\n",
        "        # Determine the dominant emotion\n",
        "        emotion = result['dominant_emotion']\n",
        "\n",
        "        # Draw rectangle around face and label with predicted emotion\n",
        "        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 0, 255), 2)\n",
        "        cv2.putText(frame, emotion, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 0, 255), 2)\n",
        "\n",
        "# Display the resulting frame (converted to RGB for display in notebooks)\n",
        "display(PIL.Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)))\n"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "qI3WSaIzigWg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}